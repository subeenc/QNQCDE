{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Positive Input IDs shape: tensor([[[ 2748,  1012,  2045,  2003,  2023,  2299,  2055,  4531,  2293,  1012,\n",
      "           2009,  2003,  1037,  2200,  3522,  2299],\n",
      "         [ 4067,  2017,  2005,  4531,  2054,  2008,  2299,  2001,  2170,  1012,\n",
      "              2,  2017,  1005,  2128,  6160,   999]],\n",
      "\n",
      "        [[ 4283,     2,  2053,  3291,     2,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0],\n",
      "         [ 2151,  2334,  5030,  2686,  1999,  6396,  1029,     2,  2045,  1005,\n",
      "           1055,  1037,  2193,  1997,  2307,  7516]],\n",
      "\n",
      "        [[ 4283, 28516,     2,  2017,  1005,  2128,  2087,  6160,     2,     0,\n",
      "              0,     0,     0,     0,     0,     0],\n",
      "         [ 2821,  1010,  1045,  2228,  1045,  2001,  3810,  1996,  3308,  2126,\n",
      "           1012,  1045,  1005,  2222,  3046,  2153]],\n",
      "\n",
      "        [[ 1045,  2215,  2070,  2592,     2,  2054,  2785,  1997,  2592,  1029,\n",
      "              2,     0,     0,     0,     0,     0],\n",
      "         [ 2054,  2079,  2017,  2113,  1029,     2,  2498,  1012,  6285,  4586,\n",
      "           1012,     2,     0,     0,     0,     0]]])\n",
      "Negative Input IDs shape: tensor([[[ 2748,  1012,  2045,  2003,  2023,  2299,  2055,  4531,  2293,  1012,\n",
      "           2009,  2003,  1037,  2200,  3522,  2299],\n",
      "         [ 4067,  2017,  2005,  4531,  2054,  2008,  2299,  2001,  2170,  1012,\n",
      "              2,  2191,  2039,  2019,  2151,  3437]],\n",
      "\n",
      "        [[ 4283,     2,  2748,     2,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0],\n",
      "         [ 2151,  2334,  5030,  2686,  1999,  6396,  1029,     2,  7592,  2129,\n",
      "           2089,  1045,  2393,  2017,  1029,     2]],\n",
      "\n",
      "        [[ 2009,  1005,  1055,  5764,  2575,  1050,  9048,  2213, 13642, 21183,\n",
      "           5555,  2047,  2259,     2,  2017,  1005],\n",
      "         [ 1045,  2342,  2000,  2275,  1037, 14764,  1012,     2,  2053,  3291,\n",
      "              2,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[ 1045,  2215,  2070,  2592,     2,  7592,  2129,  2089,  1045,  2393,\n",
      "           2017,  1029,     2,     0,     0,     0],\n",
      "         [ 2054,  2079,  2017,  2113,  1029,     2,  2009,  1025,  1055,  6206,\n",
      "           2000,  2224,  5850,  1998,  6544,  2064]]])\n",
      "Batch 2\n",
      "Positive Input IDs shape: tensor([[[ 2053,  2008,  2003,  2009,  1010,  4067,  2017,  1012,     2,  2031,\n",
      "           1037,  2204,  2154,   999,     2,     0],\n",
      "         [ 6415,  1012,     2,  2054,  2003,  2115,  3160,  1029,     2,     0,\n",
      "              0,     0,     0,     0,     0,     0],\n",
      "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[ 2064,  2017,  2393,  2033,  2424,  2054,  1996,  6493,  2518,  1999,\n",
      "           1996,  2088,  2003,  1012,     2,  1045],\n",
      "         [ 1045,  2215,  2000,  2424,  1996,  6493,  7488,  1999,  1996,  2088,\n",
      "           1012,     2,  2079,  2017,  2215,  2000],\n",
      "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[ 2053,  1010,  2008,  2003,  2009,  1010,  4283,  2061,  2172,   999,\n",
      "              2,  2017,  1005,  2128,  6160,  1012],\n",
      "         [ 2748,  1010,  1045,  2215,  2000,  5587,  1037,  2117,  2193,  1012,\n",
      "              2,  7929,  1012,  2054,  1005,  1055],\n",
      "         [ 3782, 13208,     2,  2017,  2525,  2031,  1037,  2193,  2005,  2008,\n",
      "           3967,  1012,  2079,  2017,  2215,  2000]],\n",
      "\n",
      "        [[ 2064,  2017,  2156,  2129,  2172,  1045,  2572, 11095,  2169,  3204,\n",
      "           2006,  2026,  2951,  2933,  1029,     2],\n",
      "         [12476,  1012,  4283,     2,  2017,  1005,  2128,  2200,  6160,   999,\n",
      "              2,     0,     0,     0,     0,     0],\n",
      "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]]])\n",
      "Negative Input IDs shape: tensor([[[ 2053,  2008,  2003,  2009,  1010,  4067,  2017,  1012,     2,  2017,\n",
      "           1005,  2128,  6160,  1012,     2,     0],\n",
      "         [ 6415,  1012,     2,  2129,  2064,  1045,  2393,     2,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0],\n",
      "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[ 4283,     2,  1045,  2064,  2393,  2007,  2088,  2636,  1012,  2054,\n",
      "           2828,  1997,  8875,  2024,  2017,  3331],\n",
      "         [ 2054,  2842,  2000, 12391,  1029,     2,  2079,  2017,  2215,  2000,\n",
      "           2113,  1996,  6493,  2828,  1997,  7488],\n",
      "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[ 2053,  1010,  2008,  2003,  2009,  1010,  4283,  2061,  2172,   999,\n",
      "              2,  1021,  1024,  2321,  1998,  2184],\n",
      "         [ 2748,  1010,  1045,  2215,  2000,  5587,  1037,  2117,  2193,  1012,\n",
      "              2,  2039,  2000,  2176,  1010,  2061],\n",
      "         [ 3782, 13208,     2,  2007,  1037,  2614,  1029,     2,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[ 2074,  1996,  2028,  2005,  4826,  1998,  4465,  1010,  1045,  2123,\n",
      "           1005,  1056,  2215,  2000,  5256,  2039],\n",
      "         [ 2748,  3531,  2689,  2009,  2000,  6273,     2,  2017,  1005,  2128,\n",
      "           2200,  6160,   999,     2,     0,     0],\n",
      "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0]]])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob2\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import config\n",
    "from plato.configuration_plato import PlatoConfig\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DialogueFeatures():\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, role_ids, turn_ids=None, position_ids=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.role_ids = role_ids\n",
    "        self.turn_ids = turn_ids\n",
    "        self.position_ids = position_ids\n",
    "\n",
    "        self.batch_size = len(self.input_ids)\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        data: 각 대화 샘플을 포함하는 리스트.\n",
    "        예시)\n",
    "        각 대화 샘플은 positive 대화 2개와 negative 대화 2개를 포함하는 리스트이다.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        df = pd.read_csv(file_path)[:10]  # 확인용으로 10개만 추출\n",
    "        self.len_data = len(df)\n",
    "        self.positive_pairs = []\n",
    "        self.negative_pairs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            pair = row['pairs'].split('||')\n",
    "            positive_pair = pair[0].split('|')  # Positive pair 분리\n",
    "            # print(positive_pair)\n",
    "            negative_pair = pair[1].split('|')[:len(positive_pair)]  # Negative pair 분리 (positive pair와 개수 동일하게)\n",
    "            label = [1] * len(positive_pair) + [0] * len(negative_pair)\n",
    "            \n",
    "            self.positive_pairs.append(positive_pair)\n",
    "            self.negative_pairs.append(negative_pair)\n",
    "            self.labels.append(label)\n",
    "        \n",
    "        self.positive_features = self.get_dialfeature(self.positive_pairs)\n",
    "        self.negative_features = self.get_dialfeature(self.negative_pairs)\n",
    "        # print(len(self.positive_features))\n",
    "        # print(self.positive_features)\n",
    "        # print(self.positive_pairs)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def get_dialfeature(self, pairs):  # pos, neg 각각 실행\n",
    "        \"\"\"\n",
    "        pairs 예시\n",
    "        \n",
    "        [\n",
    "            ['new contact add#name', \"5''1'#eye color\"],\n",
    "            [\"No, that is it, thanks so much!#You're welcome.\", \"5''1'#eye color\"]\n",
    "        ]\n",
    "        \"\"\"                \n",
    "\n",
    "        features = []\n",
    "        for data_index, pair in enumerate(pairs):\n",
    "            bert_features_pair = []  # 대화 내 pair 여러개이므로 구분해주기 위해 list of list\n",
    "                     \n",
    "            for t, turn in enumerate(pair):\n",
    "                \n",
    "                use_response = False\n",
    "  \n",
    "                # 하나의 pair에 대한 role\n",
    "                role_list = [1, 0]\n",
    "\n",
    "                sample_input_ids = []\n",
    "                sample_segment_ids = []\n",
    "                sample_role_ids = []\n",
    "                sample_input_mask = []\n",
    "                sample_turn_ids = []\n",
    "                sample_position_ids = [] \n",
    "                sample_tokens = [] \n",
    "\n",
    "                text_tokens = []\n",
    "                text_turn_ids = []\n",
    "                text_role_ids = []\n",
    "                text_segment_ids = []\n",
    "                    \n",
    "                # config.turn_sep_token='#'\n",
    "                text_list = turn.split('#')\n",
    "\n",
    "                # token: token [eou] token [eou] [bos] token [eos]\n",
    "                # role:   0     0     1     1     0     0      0\n",
    "                # turn:   2     2     1     1     0     0      0\n",
    "                # pos:    0     1     0     1     0     1      2\n",
    "                bou, eou, bos, eos = \"[unused0]\", \"[unused1]\", \"[unused0]\", \"[unused1]\"\n",
    "\n",
    "                # use [CLS] as the latent variable of PLATO\n",
    "                # text_list[0] = self.args.start_token + ' ' + text_list[0]\n",
    "\n",
    "                if use_response == True:   # specify the context and response\n",
    "                    context, response = text_list[:-1], text_list[-1]\n",
    "                    word_list = self.tokenizer.tokenize(response)\n",
    "                    uttr_len = len(word_list)\n",
    "\n",
    "                    start_token, end_token = bou, eou\n",
    "\n",
    "                    role_id, turn_id = role_list[-1], 0\n",
    "\n",
    "                    response_tokens = [start_token] + word_list + [end_token]\n",
    "                    response_role_ids = [role_id] * (1 + uttr_len + 1)\n",
    "                    response_turn_ids = [turn_id] * (1 + uttr_len + 1)\n",
    "                    response_segment_ids = [0] * (1 + uttr_len + 1)                   # not use\n",
    "\n",
    "                else:\n",
    "                    context = text_list\n",
    "                    response_tokens, response_role_ids, response_turn_ids, response_segment_ids = [], [], [], []\n",
    "\n",
    "                    # limit the context length\n",
    "                    # context = context[-self.args.max_context_length:]\n",
    "                    # context = context[-16:]  # 가장 최근 512개 턴만 유지(특정 길이 데이터 제한, hyperparameter 변경 필요)\n",
    "\n",
    "                    '''\n",
    "                    use_response == False일 경우, 한 대화(샘플)에서 분리된 턴이 하나씩 들어감\n",
    "                    \n",
    "                    '''\n",
    "                    # 한 turn씩 반복\n",
    "                    for i, text in enumerate(context):\n",
    "                        # print(text)\n",
    "                        word_list = self.tokenizer.tokenize(text)\n",
    "                        uttr_len = len(word_list)\n",
    "\n",
    "                        end_token = eou\n",
    "\n",
    "                        role_id, turn_id = role_list[i], len(context) - i\n",
    "\n",
    "                        text_tokens.extend(word_list + [end_token])\n",
    "                        text_role_ids.extend([role_id] * (uttr_len + 1))\n",
    "                        text_turn_ids.extend([turn_id] * (uttr_len + 1))\n",
    "                        text_segment_ids.extend([0] * (uttr_len + 1))\n",
    "\n",
    "                    \n",
    "                    text_tokens.extend(response_tokens)\n",
    "                    text_role_ids.extend(response_role_ids)\n",
    "                    text_turn_ids.extend(response_turn_ids)\n",
    "                    text_segment_ids.extend(response_segment_ids)\n",
    "\n",
    "                    if len(text_tokens) > 16:\n",
    "                        text_tokens = text_tokens[:16]\n",
    "                        text_turn_ids = text_turn_ids[:16]\n",
    "                        text_role_ids = text_role_ids[:16]\n",
    "                        text_segment_ids = text_segment_ids[:16]\n",
    "\n",
    "                    #  max_context_length=15\n",
    "                    assert (max(text_turn_ids) <= 15)\n",
    "\n",
    "                    # 制作text_position_id序列  -> Make text_position_id sequence\n",
    "                    text_position_ids = []\n",
    "                    text_position_id = 0\n",
    "                    for i, turn_id in enumerate(text_turn_ids):\n",
    "                        # print(i, turn_id)\n",
    "                        if i != 0 and turn_id < text_turn_ids[i - 1]:   # PLATO\n",
    "                            text_position_id = 0\n",
    "                        # print(text_position_id)\n",
    "                        text_position_ids.append(text_position_id)\n",
    "                        text_position_id += 1\n",
    "                    \n",
    "\n",
    "                    # max_turn_id = max(text_turn_ids)\n",
    "                    # text_turn_ids = [max_turn_id - t for t in text_turn_ids]\n",
    "\n",
    "                    text_input_ids = self.tokenizer.convert_tokens_to_ids(text_tokens)\n",
    "                    text_input_mask = [1] * len(text_input_ids)\n",
    "                    \n",
    "\n",
    "                    # Zero-pad up to the sequence length.\n",
    "                    while len(text_input_ids) < 16:\n",
    "                        text_input_ids.append(0)\n",
    "                        text_turn_ids.append(0)\n",
    "                        text_role_ids.append(0)\n",
    "                        text_segment_ids.append(0)\n",
    "                        text_position_ids.append(0)\n",
    "                        text_input_mask.append(0)\n",
    "\n",
    "                    # max_context_lengt=512\n",
    "                    assert len(text_input_ids) == 16\n",
    "                    assert len(text_turn_ids) == 16\n",
    "                    assert len(text_role_ids) == 16\n",
    "                    assert len(text_segment_ids) ==16\n",
    "                    assert len(text_position_ids) == 16\n",
    "                    assert len(text_input_mask) == 16\n",
    "                    \n",
    "                    sample_input_ids.append(text_input_ids)\n",
    "                    sample_turn_ids.append(text_turn_ids)\n",
    "                    sample_role_ids.append(text_role_ids)\n",
    "                    sample_segment_ids.append(text_segment_ids)\n",
    "                    sample_position_ids.append(text_position_ids)\n",
    "                    sample_input_mask.append(text_input_mask)\n",
    "                    sample_tokens.append(text_tokens)\n",
    "\n",
    "                bert_feature = DialogueFeatures(input_ids=sample_input_ids,\n",
    "                                            input_mask=sample_input_mask,\n",
    "                                            segment_ids=sample_segment_ids,\n",
    "                                            role_ids=sample_role_ids,\n",
    "                                            turn_ids=sample_turn_ids,\n",
    "                                            position_ids=sample_position_ids)\n",
    "                \n",
    "                bert_features_pair.append(bert_feature)\n",
    "                # print(bert_features_pair)\n",
    "            features.append(bert_features_pair)\n",
    "        return features\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 대화 샘플을 반환한다.\n",
    "        각 대화 샘플은 positive 대화 2개와 negative 대화 2개를 포함한다.\n",
    "        \"\"\"\n",
    "        # 선택된 대화 샘플을 반환\n",
    "        positive_features = self.positive_features[idx]        \n",
    "        negative_features = self.negative_features[idx]\n",
    "        \n",
    "        import itertools\n",
    "\n",
    "        # positive_features는 DialogueFeatures 객체의 리스트\n",
    "        all_pos_input_ids = list(itertools.chain.from_iterable(feature.input_ids for feature in positive_features))\n",
    "        all_pos_input_mask = list(itertools.chain.from_iterable(feature.input_mask for feature in positive_features))\n",
    "        all_pos_segment_ids = list(itertools.chain.from_iterable(feature.segment_ids for feature in positive_features))\n",
    "        all_pos_role_ids = list(itertools.chain.from_iterable(feature.role_ids for feature in positive_features))\n",
    "        all_pos_turn_ids = list(itertools.chain.from_iterable(feature.turn_ids for feature in positive_features))\n",
    "        all_pos_position_ids = list(itertools.chain.from_iterable(feature.position_ids for feature in positive_features))\n",
    "        \n",
    "        all_neg_input_ids = list(itertools.chain.from_iterable(feature.input_ids for feature in negative_features))\n",
    "        all_neg_input_mask = list(itertools.chain.from_iterable(feature.input_mask for feature in negative_features))\n",
    "        all_neg_segment_ids = list(itertools.chain.from_iterable(feature.segment_ids for feature in negative_features))\n",
    "        all_neg_role_ids = list(itertools.chain.from_iterable(feature.role_ids for feature in negative_features))\n",
    "        all_neg_turn_ids = list(itertools.chain.from_iterable(feature.turn_ids for feature in negative_features))\n",
    "        all_neg_position_ids = list(itertools.chain.from_iterable(feature.position_ids for feature in negative_features))\n",
    "        \n",
    "\n",
    "        \n",
    "        # 하나의 대화 샘플에 대한 정보 반환\n",
    "        inputs = {'positive':{\n",
    "                        'input_ids':torch.LongTensor(all_pos_input_ids),\n",
    "                        # 'input_mask':torch.LongTensor(all_pos_input_mask),\n",
    "                        # 'segment_ids':torch.LongTensor(all_pos_segment_ids),\n",
    "                        'role_ids':torch.LongTensor(all_pos_role_ids),\n",
    "                        'turn_ids':torch.LongTensor(all_pos_turn_ids),\n",
    "                        # 'position_ids':torch.LongTensor(all_pos_position_ids)\n",
    "                    },\n",
    "                  'negative':{\n",
    "                        'input_ids':torch.LongTensor(all_neg_input_ids),\n",
    "                        # 'input_mask':torch.LongTensor(all_neg_input_mask),\n",
    "                        # 'segment_ids':torch.LongTensor(all_neg_segment_ids),\n",
    "                        'role_ids':torch.LongTensor(all_neg_role_ids),\n",
    "                        'turn_ids':torch.LongTensor(all_neg_turn_ids),\n",
    "                        # 'position_ids':torch.LongTensor(all_neg_position_ids)\n",
    "                    }\n",
    "                }\n",
    "        # print(\"==========inputs===========\")\n",
    "        # print(inputs)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이는 전체 대화 샘플의 수\n",
    "        return self.len_data\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = glob2.glob(\"../dial2vec/datasets/*_train*.csv\")\n",
    "\n",
    "# CustomDataset 인스턴스 생성\n",
    "dataset = DialogueDataset(dataset_path[3])  # sgd\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # batch는 리스트 형태이며, 각 원소는 __getitem__에서 반환된 결과입니다.\n",
    "    # 각 특성별로 패딩을 적용합니다.\n",
    "    pos_input_ids = pad_sequence([torch.tensor(sample['positive']['input_ids']) for sample in batch], batch_first=True, padding_value=0)\n",
    "    pos_role_ids = pad_sequence([torch.tensor(sample['positive']['role_ids']) for sample in batch], batch_first=True, padding_value=0)\n",
    "    pos_turn_ids = pad_sequence([torch.tensor(sample['positive']['turn_ids']) for sample in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    neg_input_ids = pad_sequence([torch.tensor(sample['negative']['input_ids']) for sample in batch], batch_first=True, padding_value=0)\n",
    "    neg_role_ids = pad_sequence([torch.tensor(sample['negative']['role_ids']) for sample in batch], batch_first=True, padding_value=0)\n",
    "    neg_turn_ids = pad_sequence([torch.tensor(sample['negative']['turn_ids']) for sample in batch], batch_first=True, padding_value=0)\n",
    "\n",
    "    # 모든 특성에 대한 패딩 적용 후 최종 배치 데이터 구성\n",
    "    batched_data = {\n",
    "        'positive': {\n",
    "            'input_ids': pos_input_ids,\n",
    "            'role_ids': pos_role_ids,\n",
    "            'turn_ids': pos_turn_ids\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': neg_input_ids,\n",
    "            'role_ids': neg_role_ids,\n",
    "            'turn_ids': neg_turn_ids\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "dialogue_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "for batch_idx, batch_data in enumerate(dialogue_dataloader):\n",
    "    # batch_data에서 'positive'와 'negative' 데이터 추출\n",
    "    positive_inputs = batch_data['positive']\n",
    "    negative_inputs = batch_data['negative']\n",
    "\n",
    "        # 단순히 배치 데이터의 형태를 확인하기 위한 출력\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Positive Input IDs shape: {positive_inputs['input_ids']}\")\n",
    "    print(f\"Negative Input IDs shape: {negative_inputs['input_ids']}\")\n",
    "    # 추가적으로 필요한 처리를 여기에 구현할 수 있습니다.\n",
    "    \n",
    "    # 데모를 위한 break; 실제 사용시에는 제거해야 합니다.\n",
    "    if batch_idx == 1:  # 첫 번째 배치만 처리하고 멈추기\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# 경고창 숨기기\n",
    "warnings.filterwarnings('ignore') \n",
    "# 경고창 다시 나타내기\n",
    "# warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_kobert_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
    "    def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
    "        bertmodel = BertModel.from_pretrained(model_path, return_dict=False)\n",
    "        device = torch.device(ctx)\n",
    "        bertmodel.to(device)\n",
    "        bertmodel.eval()\n",
    "        vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "            vocab_file, padding_token=\"[PAD]\"\n",
    "        )\n",
    "        return bertmodel, vocab_b_obj\n",
    "\n",
    "    pytorch_kobert = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/models/kobert_v1.zip\",\n",
    "        \"chksum\": \"411b242919\",  # 411b2429199bc04558576acdcac6d498\n",
    "    }\n",
    "\n",
    "    # download model\n",
    "    model_info = pytorch_kobert\n",
    "    model_path, is_cached = download(\n",
    "        model_info[\"url\"], model_info[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.expanduser(cachedir)\n",
    "    zipf = ZipFile(os.path.expanduser(model_path))\n",
    "    zipf.extractall(path=cachedir_full)\n",
    "    model_path = os.path.join(os.path.expanduser(cachedir), \"kobert_from_pretrained\")\n",
    "    # download vocab\n",
    "    vocab_path = get_tokenizer()\n",
    "    return get_kobert_model(model_path, vocab_path, ctx)\n",
    "\n",
    "\n",
    "def get_loader(args, metric):\n",
    "    bert_model, vocab = get_pytorch_kobert_model()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "    \n",
    "    self.tokenizer_config = PlatoConfig.from_json_file(\"plato/config.json\")\n",
    "\n",
    "    path_to_train_data = args.path_to_data + '/' + args.task + '/' + args.train_data\n",
    "    path_to_valid_data = args.path_to_data + '/' + args.task + '/' + args.valid_data\n",
    "    path_to_test_data = args.path_to_data + '/' + args.task + '/' + args.test_data\n",
    "\n",
    "    if args.train == 'True' and args.test == 'False':\n",
    "        train_iter = ModelDataLoader(path_to_train_data, args, metric, tokenizer, vocab, type='train')\n",
    "        valid_iter = ModelDataLoader(path_to_valid_data, args, metric, tokenizer, vocab, type='valid')\n",
    "\n",
    "        train_iter.load_data('train')\n",
    "        valid_iter.load_data('valid')\n",
    "\n",
    "        loader = {'train': DataLoader(dataset=train_iter,\n",
    "                                      batch_size=args.batch_size,\n",
    "                                      shuffle=True),\n",
    "                  'valid': DataLoader(dataset=valid_iter,\n",
    "                                      batch_size=args.batch_size,\n",
    "                                      shuffle=True)}\n",
    "\n",
    "    elif args.train == 'False' and args.test == 'True':\n",
    "        test_iter = ModelDataLoader(path_to_test_data, args, metric, tokenizer, vocab, type='test')\n",
    "        test_iter.load_data('test')\n",
    "\n",
    "        loader = {'test': DataLoader(dataset=test_iter,\n",
    "                                     batch_size=args.batch_size,\n",
    "                                     shuffle=True)}\n",
    "\n",
    "    else:\n",
    "        loader = None\n",
    "\n",
    "    return bert_model, loader, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grooming_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
